{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical QA Fine-Tuning Results Analysis\n",
    "\n",
    "This notebook analyzes the results of fine-tuning LLaMA 3.1 8B on the HealthCareMagic-100K medical QA dataset. It compares the fine-tuned model with the baseline model, analyzes error patterns, and visualizes performance across medical specialties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Plotting setup\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evaluation Results\n",
    "\n",
    "First, let's load the evaluation results for both the fine-tuned and baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Helper function to load evaluation results\n",
    "def load_eval_results(eval_dir):\n",
    "    \"\"\"\n",
    "    Load evaluation metrics and predictions.\n",
    "    \n",
    "    Args:\n",
    "        eval_dir: Evaluation directory\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing metrics and predictions\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Load metrics\n",
    "    metrics_file = os.path.join(eval_dir, \"metrics.json\")\n",
    "    if os.path.exists(metrics_file):\n",
    "        with open(metrics_file, \"r\") as f:\n",
    "            results[\"metrics\"] = json.load(f)\n",
    "    \n",
    "    # Load predictions\n",
    "    predictions_file = os.path.join(eval_dir, \"predictions.jsonl\")\n",
    "    if os.path.exists(predictions_file):\n",
    "        predictions = []\n",
    "        with open(predictions_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                predictions.append(json.loads(line))\n",
    "        results[\"predictions\"] = predictions\n",
    "    \n",
    "    # Load comparison if available\n",
    "    comparison_file = os.path.join(eval_dir, \"model_comparison.md\")\n",
    "    if os.path.exists(comparison_file):\n",
    "        with open(comparison_file, \"r\") as f:\n",
    "            results[\"comparison\"] = f.read()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Path to latest evaluation results\n",
    "eval_base_dir = \"../outputs/evaluation\"\n",
    "\n",
    "# Find latest evaluation directory\n",
    "eval_dirs = glob.glob(os.path.join(eval_base_dir, \"*\"))\n",
    "eval_dirs = [d for d in eval_dirs if os.path.isdir(d) and d != os.path.join(eval_base_dir, \"latest\")]\n",
    "latest_eval_dir = max(eval_dirs, key=os.path.getctime) if eval_dirs else None\n",
    "\n",
    "if latest_eval_dir:\n",
    "    print(f\"Loading results from {latest_eval_dir}\")\n",
    "    \n",
    "    # Load fine-tuned model results\n",
    "    finetuned_results = load_eval_results(latest_eval_dir)\n",
    "    \n",
    "    # Load baseline model results if available\n",
    "    baseline_dir = os.path.join(latest_eval_dir, \"baseline\")\n",
    "    baseline_results = load_eval_results(baseline_dir) if os.path.exists(baseline_dir) else None\n",
    "    \n",
    "    print(f\"Loaded evaluation results:\")\n",
    "    print(f\"  - Fine-tuned model: {len(finetuned_results.get('metrics', {}))} metrics, {len(finetuned_results.get('predictions', []))} predictions\")\n",
    "    if baseline_results:\n",
    "        print(f\"  - Baseline model: {len(baseline_results.get('metrics', {}))} metrics, {len(baseline_results.get('predictions', []))} predictions\")\n",
    "else:\n",
    "    print(\"No evaluation results found. Please run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Overview\n",
    "\n",
    "Let's compare the key metrics between the fine-tuned and baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_metric_comparison(finetuned_metrics, baseline_metrics=None, metric_groups=None):\n",
    "    \"\"\"\n",
    "    Plot comparison of metrics between fine-tuned and baseline models.\n",
    "    \n",
    "    Args:\n",
    "        finetuned_metrics: Fine-tuned model metrics\n",
    "        baseline_metrics: Baseline model metrics (optional)\n",
    "        metric_groups: Dictionary mapping group names to lists of metric keys\n",
    "    \"\"\"\n",
    "    if metric_groups is None:\n",
    "        # Default metric groups\n",
    "        metric_groups = {\n",
    "            \"Text Generation\": [\"rouge_1\", \"rouge_2\", \"rouge_l\", \"bleu\"],\n",
    "            \"Semantic Similarity\": [\"bertscore_precision\", \"bertscore_recall\", \"bertscore_f1\"],\n",
    "            \"Medical Domain\": [\"diagnosis_presence_match_rate\", \"treatment_presence_match_rate\", \n",
    "                               \"structure_preservation_rate\", \"medical_terminology_ratio\"],\n",
    "        }\n",
    "    \n",
    "    # Iterate through metric groups\n",
    "    for group_name, metrics in metric_groups.items():\n",
    "        # Filter metrics that exist in both results\n",
    "        available_metrics = [m for m in metrics if m in finetuned_metrics]\n",
    "        if not available_metrics:\n",
    "            continue\n",
    "        \n",
    "        # Create DataFrame for comparison\n",
    "        if baseline_metrics is not None:\n",
    "            # With baseline comparison\n",
    "            available_metrics = [m for m in available_metrics if m in baseline_metrics]\n",
    "            if not available_metrics:\n",
    "                continue\n",
    "                \n",
    "            df = pd.DataFrame({\n",
    "                \"Metric\": [m.replace(\"_\", \" \").title() for m in available_metrics],\n",
    "                \"Baseline\": [baseline_metrics[m] for m in available_metrics],\n",
    "                \"Fine-tuned\": [finetuned_metrics[m] for m in available_metrics],\n",
    "            })\n",
    "            \n",
    "            # Calculate improvement\n",
    "            df[\"Improvement (%)\"] = ((df[\"Fine-tuned\"] - df[\"Baseline\"]) / df[\"Baseline\"] * 100).fillna(0)\n",
    "            \n",
    "            # Plot comparison\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "            \n",
    "            # Plot metric values\n",
    "            df_plot = df[[\"Metric\", \"Baseline\", \"Fine-tuned\"]].melt(id_vars=\"Metric\", var_name=\"Model\", value_name=\"Value\")\n",
    "            sns.barplot(data=df_plot, x=\"Metric\", y=\"Value\", hue=\"Model\", ax=ax1)\n",
    "            ax1.set_title(f\"{group_name} Metrics: Baseline vs. Fine-tuned\"