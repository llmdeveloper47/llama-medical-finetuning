{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HealthCareMagic-100K Dataset Exploration\n",
    "\n",
    "This notebook explores the HealthCareMagic-100K dataset to understand its structure, content, and prepare it for fine-tuning a medical QA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from wordcloud import WordCloud\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set figure size\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load HealthCareMagic-100K Dataset\n",
    "\n",
    "The HealthCareMagic-100K dataset contains 100,000+ patient questions and doctor answers from a medical consultation website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Try loading the dataset directly from Hugging Face\n",
    "try:\n",
    "    dataset = load_dataset(\"vaibhavs10/healthcaremagic-100k\")\n",
    "    print(f\"Dataset loaded from Hugging Face: {dataset}\")\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    train_df = pd.DataFrame(dataset[\"train\"])\n",
    "    test_df = pd.DataFrame(dataset[\"test\"])\n",
    "    \n",
    "    print(f\"Training set: {len(train_df)} examples\")\n",
    "    print(f\"Test set: {len(test_df)} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset from Hugging Face: {e}\")\n",
    "    \n",
    "    # Try loading from local files\n",
    "    try:\n",
    "        data_dir = \"../data/processed\"\n",
    "        \n",
    "        # Check if processed files exist\n",
    "        if os.path.exists(os.path.join(data_dir, \"train.jsonl\")):\n",
    "            with open(os.path.join(data_dir, \"train.jsonl\"), \"r\") as f:\n",
    "                train_data = [json.loads(line) for line in f]\n",
    "            \n",
    "            with open(os.path.join(data_dir, \"validation.jsonl\"), \"r\") as f:\n",
    "                val_data = [json.loads(line) for line in f]\n",
    "                \n",
    "            with open(os.path.join(data_dir, \"test.jsonl\"), \"r\") as f:\n",
    "                test_data = [json.loads(line) for line in f]\n",
    "            \n",
    "            train_df = pd.DataFrame(train_data)\n",
    "            val_df = pd.DataFrame(val_data)\n",
    "            test_df = pd.DataFrame(test_data)\n",
    "            \n",
    "            print(f\"Training set: {len(train_df)} examples\")\n",
    "            print(f\"Validation set: {len(val_df)} examples\")\n",
    "            print(f\"Test set: {len(test_df)} examples\")\n",
    "        else:\n",
    "            print(\"Processed data files not found. Please run the data preparation script first.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Error loading local dataset: {e2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display sample examples\n",
    "print(\"Training sample:\")\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Column information\n",
    "print(\"Dataset columns:\")\n",
    "for col in train_df.columns:\n",
    "    print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Data Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Text length statistics\n",
    "def get_text_stats(df, text_column):\n",
    "    lengths = df[text_column].str.split().str.len()\n",
    "    return {\n",
    "        \"mean\": lengths.mean(),\n",
    "        \"median\": lengths.median(),\n",
    "        \"min\": lengths.min(),\n",
    "        \"max\": lengths.max(),\n",
    "        \"90th_percentile\": lengths.quantile(0.9),\n",
    "        \"95th_percentile\": lengths.quantile(0.95),\n",
    "        \"99th_percentile\": lengths.quantile(0.99),\n",
    "    }\n",
    "\n",
    "# Input text statistics\n",
    "input_stats = get_text_stats(train_df, \"input\")\n",
    "print(\"Input text statistics (word count):\")\n",
    "for key, value in input_stats.items():\n",
    "    print(f\"- {key}: {value:.1f}\")\n",
    "\n",
    "# Output text statistics\n",
    "output_stats = get_text_stats(train_df, \"output\")\n",
    "print(\"\\nOutput text statistics (word count):\")\n",
    "for key, value in output_stats.items():\n",
    "    print(f\"- {key}: {value:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot text length distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Input length distribution\n",
    "input_lengths = train_df[\"input\"].str.split().str.len()\n",
    "sns.histplot(input_lengths.clip(upper=500), ax=axes[0], bins=50, kde=True)\n",
    "axes[0].set_title(\"Input Text Length Distribution\")\n",
    "axes[0].set_xlabel(\"Word Count\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Output length distribution\n",
    "output_lengths = train_df[\"output\"].str.split().str.len()\n",
    "sns.histplot(output_lengths.clip(upper=500), ax=axes[1], bins=50, kde=True)\n",
    "axes[1].set_title(\"Output Text Length Distribution\")\n",
    "axes[1].set_xlabel(\"Word Count\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Medical Topics and Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract common medical conditions from text\n",
    "medical_conditions = [\n",
    "    \"diabetes\", \"hypertension\", \"asthma\", \"arthritis\", \"depression\", \"anxiety\",\n",
    "    \"cancer\", \"heart disease\", \"stroke\", \"alzheimer\", \"parkinson\", \"epilepsy\",\n",
    "    \"copd\", \"flu\", \"pneumonia\", \"hiv\", \"tuberculosis\", \"malaria\", \"hepatitis\",\n",
    "    \"migraine\", \"osteoporosis\", \"thyroid\", \"lupus\", \"fibromyalgia\", \"eczema\",\n",
    "    \"psoriasis\", \"allergy\", \"infection\", \"fever\", \"pain\", \"inflammation\",\n",
    "    \"fracture\", \"injury\", \"surgery\", \"pregnancy\"\n",
    "]\n",
    "\n",
    "# Count conditions in input and output text\n",
    "def extract_conditions(text, conditions_list):\n",
    "    found_conditions = []\n",
    "    for condition in conditions_list:\n",
    "        pattern = r'\\b' + condition + r'\\b'\n",
    "        if re.search(pattern, text.lower()):\n",
    "            found_conditions.append(condition)\n",
    "    return found_conditions\n",
    "\n",
    "# Apply to the dataset\n",
    "all_conditions = []\n",
    "for text in tqdm(train_df[\"input\"] + \" \" + train_df[\"output\"]):\n",
    "    all_conditions.extend(extract_conditions(text, medical_conditions))\n",
    "\n",
    "# Count frequencies\n",
    "condition_counts = Counter(all_conditions)\n",
    "top_conditions = condition_counts.most_common(15)\n",
    "\n",
    "# Plot top conditions\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=[condition for condition, count in top_conditions],\n",
    "            y=[count for condition, count in top_conditions])\n",
    "plt.title(\"Top 15 Medical Conditions in the Dataset\")\n",
    "plt.xlabel(\"Condition\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create word clouds for input and output text\n",
    "def create_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\", \n",
    "                         max_words=100, contour_width=3, contour_color=\"steelblue\")\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud.generate(text)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Sample 1000 examples for faster processing\n",
    "sample_df = train_df.sample(1000, random_state=42)\n",
    "\n",
    "# Create word clouds\n",
    "create_wordcloud(\" \".join(sample_df[\"input\"]), \"Word Cloud of Patient Questions\")\n",
    "create_wordcloud(\" \".join(sample_df[\"output\"]), \"Word Cloud of Doctor Answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Response Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for diagnosis and treatment sections in responses\n",
    "def has_diagnosis(text):\n",
    "    return bool(re.search(r\"(?i)diagnosis|assessment|impression\", text))\n",
    "\n",
    "def has_treatment(text):\n",
    "    return bool(re.search(r\"(?i)treatment|plan|recommendation|therapy|management\", text))\n",
    "\n",
    "# Apply to the dataset\n",
    "train_df[\"has_diagnosis\"] = train_df[\"output\"].apply(has_diagnosis)\n",
    "train_df[\"has_treatment\"] = train_df[\"output\"].apply(has_treatment)\n",
    "train_df[\"has_both\"] = train_df[\"has_diagnosis\"] & train_df[\"has_treatment\"]\n",
    "\n",
    "# Calculate percentages\n",
    "diagnosis_percent = train_df[\"has_diagnosis\"].mean() * 100\n",
    "treatment_percent = train_df[\"has_treatment\"].mean() * 100\n",
    "both_percent = train_df[\"has_both\"].mean() * 100\n",
    "\n",
    "print(f\"Responses with diagnosis section: {diagnosis_percent:.1f}%\")\n",
    "print(f\"Responses with treatment section: {treatment_percent:.1f}%\")\n",
    "print(f\"Responses with both sections: {both_percent:.1f}%\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([\"Diagnosis\", \"Treatment\", \"Both\"], [diagnosis_percent, treatment_percent, both_percent])\n",
    "plt.title(\"Percentage of Responses with Diagnosis and Treatment Sections\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.ylim(0, 100)\n",
    "for i, v in enumerate([diagnosis_percent, treatment_percent, both_percent]):\n",
    "    plt.text(i, v+2, f\"{v:.1f}%\", ha=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Instruction Template Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# If we have instruction templates in the processed data\n",
    "if \"instruction\" in train_df.columns:\n",
    "    # Count different instruction templates\n",
    "    instruction_counts = train_df[\"instruction\"].value_counts()\n",
    "    \n",
    "    print(\"Instruction template distribution:\")\n",
    "    for instruction, count in instruction_counts.items():\n",
    "        print(f\"- {instruction}: {count} examples ({count/len(train_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.countplot(y=\"instruction\", data=train_df, order=instruction_counts.index)\n",
    "    plt.title(\"Distribution of Instruction Templates\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Token Lengths for Model Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    print(\"Using a different tokenizer for estimation\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Function to tokenize and get stats\n",
    "def get_token_stats(texts, tokenizer):\n",
    "    token_lengths = [len(tokenizer.encode(text)) for text in texts[:1000]]  # Sample for speed\n",
    "    return {\n",
    "        \"mean\": np.mean(token_lengths),\n",
    "        \"median\": np.median(token_lengths),\n",
    "        \"95th_percentile\": np.percentile(token_lengths, 95),\n",
    "        \"99th_percentile\": np.percentile(token_lengths, 99),\n",
    "        \"max\": np.max(token_lengths),\n",
    "    }\n",
    "\n",
    "# Combine instruction, input, and output as they would be formatted for training\n",
    "if all(col in train_df.columns for col in [\"instruction\", \"input\", \"output\"]):\n",
    "    # Sample for faster processing\n",
    "    sample_df = train_df.sample(min(1000, len(train_df)), random_state=42)\n",
    "    \n",
    "    # Format as training examples\n",
    "    formatted_examples = []\n",
    "    for _, row in sample_df.iterrows():\n",
    "        if row[\"input\"]:\n",
    "            formatted = f\"<|user|>\\n{row['instruction']}\\n\\n{row['input']}<|endofuser|>\\n<|assistant|>\\n{row['output']}<|endofassistant|>\"\n",
    "        else:\n",
    "            formatted = f\"<|user|>\\n{row['instruction']}<|endofuser|>\\n<|assistant|>\\n{row['output']}<|endofassistant|>\"\n",
    "        formatted_examples.append(formatted)\n",
    "    \n",
    "    # Get token stats\n",
    "    token_stats = get_token_stats(formatted_examples, tokenizer)\n",
    "    \n",
    "    print(\"Token length statistics for formatted examples:\")\n",
    "    for key, value in token_stats.items():\n",
    "        print(f\"- {key}: {value:.1f}\")\n",
    "    \n",
    "    # Plot token length distribution\n",
    "    token_lengths = [len(tokenizer.encode(text)) for text in formatted_examples]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(token_lengths, bins=50, kde=True)\n",
    "    plt.axvline(x=4096, color='r', linestyle='--', label='LLaMA 3.1 Context Limit (4096)')\n",
    "    plt.title(\"Token Length Distribution of Formatted Examples\")\n",
    "    plt.xlabel(\"Token Count\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check percentage exceeding context window\n",
    "    exceeding = sum(1 for length in token_lengths if length > 4096)\n",
    "    print(f\"Percentage of examples exceeding 4096 tokens: {exceeding/len(token_lengths)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Visualize Medical Specialties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define medical specialties and their associated keywords\n",
    "specialties = {\n",
    "    \"cardiology\": [\"heart\", \"cardiac\", \"myocardial\", \"infarction\", \"angina\", \"hypertension\", \"arrhythmia\"],\n",
    "    \"pulmonology\": [\"lung\", \"pulmonary\", \"respiratory\", \"asthma\", \"copd\", \"pneumonia\"],\n",
    "    \"neurology\": [\"brain\", \"neural\", \"seizure\", \"stroke\", \"dementia\", \"alzheimer\", \"parkinson\"],\n",
    "    \"gastroenterology\": [\"stomach\", \"intestine\", \"bowel\", \"liver\", \"pancreas\", \"gallbladder\", \"ulcer\"],\n",
    "    \"orthopedics\": [\"bone\", \"joint\", \"fracture\", \"arthritis\", \"osteoporosis\", \"rheumatoid\"],\n",
    "    \"endocrinology\": [\"diabetes\", \"thyroid\", \"hormone\", \"insulin\", \"adrenal\", \"pituitary\"],\n",
    "    \"infectious_disease\": [\"infection\", \"bacterial\", \"viral\", \"fungal\", \"sepsis\", \"antibiotic\"],\n",
    "    \"dermatology\": [\"skin\", \"rash\", \"eczema\", \"psoriasis\", \"acne\", \"dermatitis\"],\n",
    "    \"obstetrics_gynecology\": [\"pregnancy\", \"menstrual\", \"uterus\", \"ovary\", \"cervical\", \"ovulation\"],\n",
    "    \"psychiatry\": [\"depression\", \"anxiety\", \"bipolar\", \"schizophrenia\", \"mental\", \"psychiatric\"],\n",
    "    \"urology\": [\"kidney\", \"bladder\", \"urinary\", \"prostate\", \"urination\", \"urine\"],\n",
    "    \"oncology\": [\"cancer\", \"tumor\", \"malignant\", \"chemotherapy\", \"radiation\", \"carcinoma\"],\n",
    "}\n",
    "\n",
    "# Function to categorize text by specialty\n",
    "def categorize_by_specialty(text, specialties_dict):\n",
    "    text = text.lower()\n",
    "    matched_specialties = []\n",
    "    \n",
    "    for specialty, keywords in specialties_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(r'\\b' + keyword + r'\\b', text):\n",
    "                matched_specialties.append(specialty)\n",
    "                break\n",
    "    \n",
    "    return matched_specialties\n",
    "\n",
    "# Apply to a sample for faster processing\n",
    "sample_df = train_df.sample(min(1000, len(train_df)), random_state=42)\n",
    "\n",
    "# Extract specialties\n",
    "all_specialties = []\n",
    "for text in tqdm(sample_df[\"input\"] + \" \" + sample_df[\"output\"]):\n",
    "    all_specialties.extend(categorize_by_specialty(text, specialties))\n",
    "\n",
    "# Count frequencies\n",
    "specialty_counts = Counter(all_specialties)\n",
    "total_samples = len(sample_df)\n",
    "\n",
    "# Plot specialty distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "specialty_df = pd.DataFrame({\n",
    "    \"Specialty\": list(specialty_counts.keys()),\n",
    "    \"Count\": list(specialty_counts.values()),\n",
    "    \"Percentage\": [count/total_samples*100 for count in specialty_counts.values()]\n",
    "}).sort_values(\"Count\", ascending=False)\n",
    "\n",
    "sns.barplot(x=\"Percentage\", y=\"Specialty\", data=specialty_df)\n",
    "plt.title(\"Medical Specialties Distribution in Dataset\")\n",
    "plt.xlabel(\"Percentage of Examples (%)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Summary\n",
    "\n",
    "Based on the exploration above, here's a summary of the HealthCareMagic-100K dataset:\n",
    "\n",
    "1. **Size**: Approximately 100,000 medical consultations divided into training and testing sets\n",
    "2. **Format**: Patient questions (input) and doctor answers (output) pairs\n",
    "3. **Text Length**: \n",
    "   - Patient questions: Average of X words (Y tokens)\n",
    "   - Doctor answers: Average of X words (Y tokens)\n",
    "4. **Structure**: \n",
    "   - X% of responses contain diagnosis sections\n",
    "   - X% contain treatment recommendations\n",
    "   - X% have both diagnosis and treatment\n",
    "5. **Medical Specialties**: The dataset covers a wide range of specialties, with X, Y, and Z being the most common\n",
    "6. **Common Conditions**: The most frequently mentioned conditions include X, Y, and Z\n",
    "\n",
    "### Recommendations for Fine-tuning\n",
    "\n",
    "Based on this analysis, here are recommendations for fine-tuning LLaMA 3.1 8B on this dataset:\n",
    "\n",
    "1. **Context Length**: The model context length of 4096 tokens should be sufficient for X% of examples. For longer examples, consider truncation strategies that preserve the most important information.\n",
    "\n",
    "2. **Instruction Templates**: Use templates that explicitly prompt for diagnosis and treatment plans to encourage structured responses.\n",
    "\n",
    "3. **Response Structure**: Consider adding post-processing to ensure responses have clear diagnosis and treatment sections, as this is the format in X% of the training data.\n",
    "\n",
    "4. **Batch Size**: Given the average token length of examples, a batch size of 2-4 should be feasible on 8 A100 GPUs with gradient accumulation.\n",
    "\n",
    "5. **Evaluation Metrics**: Beyond general text generation metrics, include specialized metrics for medical response quality, such as diagnosis presence, treatment recommendation quality, and medical terminology usage.\n",
    "\n",
    "6. **Domain Coverage**: The dataset has good coverage across major medical specialties, but consider evaluating performance separately for each specialty to identify areas where the model might need improvement.\n",
    "\n",
    "7. **Data Preprocessing**: Consider structuring outputs more consistently with clear diagnosis and treatment sections for examples where these aren't explicitly marked."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
